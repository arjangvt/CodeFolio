{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenizing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNo7kkcfAWKTLAswWxy+KWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjangvt/CodeFolio/blob/main/ML/NLP/Basics/Tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here I use NLTK for basic text and language process.\n",
        "I use these examples as my own references.\n",
        "\n",
        "Note:\n",
        "Typically if you install nltk on your personal computer, you don't need to install each package separately (See NLTK installation colab file). Since Colab does not hold the installation we have to install nesessary packages separately. <br>\n",
        "Written by: Arjang Fahim Date: 3/19/2018"
      ],
      "metadata": {
        "id": "YiL3NinhIEz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing"
      ],
      "metadata": {
        "id": "hLX8lKyoICz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpXh24LuH4qe",
        "outputId": "f1e181b0-4657-44ad-9c7c-2da446904a4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dataset\n",
        "dataset = \"Hello everyon. Welcome to this course. We are studying NLP.\""
      ],
      "metadata": {
        "id": "VeOKIIsIIVIv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokanizing sentences\n",
        "print(sent_tokenize(text=dataset, language='english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qobadUtPIZjT",
        "outputId": "e31739cd-5c4c-4c17-9194-1c919f033845"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello everyon.', 'Welcome to this course.', 'We are studying NLP.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sent_tokenize(text=dataset, language='english'):\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_d6eqU4IfeV",
        "outputId": "76be1037-04d1-4782-d171-781fb270ddcd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello everyon.\n",
            "Welcome to this course.\n",
            "We are studying NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokanizing words\n",
        "print(word_tokenize(text=dataset, language='english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HntUNJS-Ih-N",
        "outputId": "bdac0d7a-4d74-46f8-9025-94f6479e7f82"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'everyon', '.', 'Welcome', 'to', 'this', 'course', '.', 'We', 'are', 'studying', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_tokenize(text=dataset, language='english'):\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYEVFr2FIkY0",
        "outputId": "841291f8-a594-41be-97e8-11f0b2859343"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "everyon\n",
            ".\n",
            "Welcome\n",
            "to\n",
            "this\n",
            "course\n",
            ".\n",
            "We\n",
            "are\n",
            "studying\n",
            "NLP\n",
            ".\n"
          ]
        }
      ]
    }
  ]
}